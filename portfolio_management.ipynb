{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOZ7xrzN8l/VIRcaE3ebek3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SidS12345/Quant-projects/blob/main/portfolio_management.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Goal - to build a portfolio optimiser\n",
        "\n",
        "This project aims to build a portfolio optimizer using historical stock data for a selection of major technology companies. We will begin by calculating logarithmic returns and annualizing both the mean returns and the covariance matrix. These will be used to estimate portfolio performance metrics, including expected return, volatility, and the Sharpe ratio. The optimizer will then apply constrained optimization to maximize the Sharpe ratio, subject to diversification limits on individual asset weightings. The final output will be a set of optimal portfolio weights that achieve the best possible trade-off between risk and return."
      ],
      "metadata": {
        "id": "WYS-j3s2hV9Q"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-53fxdYThRDL"
      },
      "outputs": [],
      "source": [
        "# importing all needed libraries for this project\n",
        "import yfinance as yf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from datetime import datetime, timedelta\n",
        "from scipy.optimize import minimize"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# choosing our tickers (stocks) to follow\n",
        "tickers = ['AAPL', 'MSFT', 'GOOGL', 'AMZN', 'TSLA']\n",
        "\n",
        "# choosing start and end date, taking today's date to be the end date and giving us 1500 days of stock data\n",
        "end_date = datetime.today()\n",
        "start_date = end_date - timedelta(days = 1500)"
      ],
      "metadata": {
        "id": "EOmXup-nXGkl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we want to create our dataframe. This will be a dataframe storing the close price of every ticker in our list on each day in our start to end range. The close price in the yfinance dataframe is actually the adjusted close price, which takes into accounts dividends etc to give us a better overall picture of the stock price movement\n"
      ],
      "metadata": {
        "id": "6KcUF-qYhUno"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "closing_prices = pd.DataFrame()\n",
        "for ticker in tickers:\n",
        "  data = yf.download(ticker, start = start_date, end = end_date)\n",
        "  closing_prices[ticker] = data[\"Close\"]\n",
        "\n",
        "# Now cleaning the data so that we can operate on it successfully\n",
        "\n",
        "closing_prices = closing_prices.dropna(how='any')\n",
        "closing_prices = closing_prices.drop_duplicates()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lEsGzdzPgCpx",
        "outputId": "f9d40c1e-824c-4c3b-a5eb-9174057db9bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-10-599713505.py:3: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  data = yf.download(ticker, start = start_date, end = end_date)\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "/tmp/ipython-input-10-599713505.py:3: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  data = yf.download(ticker, start = start_date, end = end_date)\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "/tmp/ipython-input-10-599713505.py:3: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  data = yf.download(ticker, start = start_date, end = end_date)\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "/tmp/ipython-input-10-599713505.py:3: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  data = yf.download(ticker, start = start_date, end = end_date)\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "/tmp/ipython-input-10-599713505.py:3: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  data = yf.download(ticker, start = start_date, end = end_date)\n",
            "[*********************100%***********************]  1 of 1 completed"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                  AAPL        MSFT       GOOGL        AMZN        TSLA\n",
            "Date                                                                  \n",
            "2021-06-01  121.622765  239.325455  118.349220  160.932495  207.966660\n",
            "2021-06-02  122.386086  239.228699  117.822876  161.699493  201.706665\n",
            "2021-06-03  120.898590  237.690598  116.679237  159.350494  190.946671\n",
            "2021-06-04  123.198349  242.604782  118.965027  160.311005  199.683334\n",
            "2021-06-07  123.208130  245.526230  119.398911  159.900497  201.710007\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we want to start understanding returns. We can either go for simple returns, calculating direct percentage returns from one day to the next, or we can take the logarithm of this. Here, we will use the logarithm, as it will allow us to aggregate returns over time more nicely. Also, taking logarithms will make our distribution appear normalised, which will be helpful for interpreting results"
      ],
      "metadata": {
        "id": "BakywmsB09Lm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "log_returns = np.log((closing_prices / closing_prices.shift(1)).dropna())\n",
        "\n",
        "# closing_prices.shift(1) shifts our datapoints down by 1, so that at the row for time t, we have the close price at time t-1.\n",
        "# We drop na as we will get an NaN in the first row due to the shift"
      ],
      "metadata": {
        "id": "BBjGLM3y08gL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we start performing our data analysis. The first thing we will want to do is to get information that will be useful for our optimizer."
      ],
      "metadata": {
        "id": "t1vcElPN40Hb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mean_returns = log_returns.mean() * 252         # Gives us an expected annual return on our stock\n",
        "covariance_matrix = log_returns.cov() * 252     # Gives us covariances between stocks, can help us understand how assets move together\n",
        "\n",
        "# We multiply by 252 to annualise the data - log_returns currently stores daily information\n",
        "\n",
        "# We also introduce our risk free rate variable, which will be needed in the sharpe ratio calculation\n",
        "\n",
        "risk_free_rate = 0.05"
      ],
      "metadata": {
        "id": "lQb-hC5s3pYY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We now want to analyse a portfolio more specifically. We want to look at 3 things - Expected return of a portfolio, volatility and sharpe ratio"
      ],
      "metadata": {
        "id": "nKTAkym5_qmv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def expected_portfolio_returns(weights, mean_returns):\n",
        "  return np.dot(weights, mean_returns)\n",
        "\n",
        "def volatility(weights, mean_returns, covariance_matrix):\n",
        "  return np.sqrt(np.dot(weights.T,np.dot(covariance_matrix,weights))) #standard deviation\n",
        "\n",
        "def sharpe_ratio(weights, mean_returns, covariance_matrix, risk_free_rate):\n",
        "  return (expected_portfolio_returns(weights, mean_returns) - risk_free_rate)/volatility(weights, mean_returns, covariance_matrix)"
      ],
      "metadata": {
        "id": "1EjgWNtBAlKH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, minimizing the negative sharpe ratio (to maximize the sharpe ratio)"
      ],
      "metadata": {
        "id": "BvUbkLIIFeAw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def neg_sharpe(weights, mean_returns, covariance_matrix, risk_free_rate):\n",
        "    return -sharpe_ratio(weights, mean_returns, covariance_matrix, risk_free_rate)\n",
        "\n",
        "max_stock_percentage = 0.4 # maximum percentage of our portfolio that can be put on a single stock, to ensure that we don't put all our money into one stock\n",
        "num_of_stocks = len(tickers)\n",
        "constraints = ({'type': 'eq', 'fun': lambda w: np.sum(w) - 1})\n",
        "bounds = [(0,max_stock_percentage) for i in range(num_of_stocks)]\n",
        "\n",
        "initial_weights = [1/num_of_stocks for i in range(num_of_stocks)]\n",
        "\n",
        "optimised_results = minimize(neg_sharpe,initial_weights, args = (mean_returns, covariance_matrix, risk_free_rate), method = \"SLSQP\", bounds = bounds, constraints = constraints)\n",
        "optimal_weights = optimised_results.x"
      ],
      "metadata": {
        "id": "HhOBl35JFhwl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Analysing the portfolio using these optimal weights"
      ],
      "metadata": {
        "id": "tRf4-VnX_c8N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Optimal weights for portfolio\")\n",
        "for ticker, weight in zip(tickers, optimal_weights):\n",
        "  print(f\"{ticker} weight: {weight:.3f}\")\n",
        "\n",
        "optimal_portfolio_return = expected_portfolio_returns(optimal_weights, mean_returns)\n",
        "optimal_portfolio_volatility = volatility(optimal_weights, mean_returns, covariance_matrix)\n",
        "optimal_sharpe_ratio = sharpe_ratio(optimal_weights, mean_returns, covariance_matrix, risk_free_rate)\n",
        "\n",
        "print(f\"Expected Annual Return: {optimal_portfolio_return:.3f}\")\n",
        "print(f\"Expected Volatility: {optimal_portfolio_volatility:.3f}\")\n",
        "print(f\"Sharpe Ratio: {optimal_sharpe_ratio:.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cbfLdYnmASKm",
        "outputId": "0986fb28-5955-409e-c2f0-93837367ac2e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimal weights for portfolio\n",
            "AAPL weight: 0.400\n",
            "MSFT weight: 0.400\n",
            "GOOGL weight: 0.200\n",
            "AMZN weight: 0.000\n",
            "TSLA weight: 0.000\n",
            "Expected Annual Return: 0.146\n",
            "Expected Volatility: 0.248\n",
            "Sharpe Ratio: 0.387\n"
          ]
        }
      ]
    }
  ]
}