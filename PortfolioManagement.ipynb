{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WYS-j3s2hV9Q"
   },
   "source": [
    "Aim - to build a portfolio optimiser\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n"
     ]
    }
   ],
   "source": [
    "print(\"hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "-53fxdYThRDL"
   },
   "outputs": [],
   "source": [
    "# importing all needed libraries\n",
    "import yfinance as yf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "from scipy.optimize import minimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "EOmXup-nXGkl"
   },
   "outputs": [],
   "source": [
    "# choosing our tickers (stocks) to follow\n",
    "# tickers = ['AAPL', 'MSFT', 'GOOGL', 'AMZN', 'TSLA']\n",
    "\n",
    "tickers = ['AAPL']\n",
    "\n",
    "# choosing start and end date, taking today's date to be the end date and giving us 1500 days of stock data\n",
    "end_date = datetime.today()\n",
    "start_date = end_date - timedelta(days = 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6KcUF-qYhUno"
   },
   "source": [
    "Now, we want to create our dataframe. This will be a dataframe storing the close price of every ticker in our list on each day in our start to end range. The close price in the yfinance dataframe is actually the adjusted close price, which takes into accounts dividends etc to give us a better overall picture of the stock price movement\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lEsGzdzPgCpx",
    "outputId": "f9d40c1e-824c-4c3b-a5eb-9174057db9bf"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n",
      "\n",
      "1 Failed download:\n",
      "['AAPL']: YFRateLimitError('Too Many Requests. Rate limited. Try after a while.')\n"
     ]
    }
   ],
   "source": [
    "closing_prices = pd.DataFrame()\n",
    "for ticker in tickers:\n",
    "  data = yf.download(ticker, start = start_date, end = end_date, auto_adjust = True)\n",
    "  closing_prices[ticker] = data[\"Close\"]\n",
    "\n",
    "# Now cleaning the data so that we can operate on it successfully\n",
    "\n",
    "closing_prices = closing_prices.dropna(how='any')\n",
    "closing_prices = closing_prices.drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BakywmsB09Lm"
   },
   "source": [
    "Next, we want to start understanding returns. We can either go for simple returns, calculating direct percentage returns from one day to the next, or we can take the logarithm of this. Here, we will use log returns, as it will allow us to aggregate returns over time more nicely. Also, taking logarithms will make our distribution appear normalised, which will be helpful for interpreting results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BBjGLM3y08gL"
   },
   "outputs": [],
   "source": [
    "log_returns = np.log((closing_prices / closing_prices.shift(1)).dropna())\n",
    "\n",
    "# closing_prices.shift(1) shifts our datapoints down by 1, so that at the row for time t, we have the close price at time t-1.\n",
    "# We drop na as we will get an NaN in the first row due to the shift"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t1vcElPN40Hb"
   },
   "source": [
    "Now, we start performing our data analysis. The first thing we will want to do is to get information that will be useful for our optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lQb-hC5s3pYY"
   },
   "outputs": [],
   "source": [
    "mean_returns = log_returns.mean() * 252         # Gives us an expected annual return on our stock\n",
    "covariance_matrix = log_returns.cov() * 252     # Gives us covariances between stocks, can help us understand how assets move together\n",
    "\n",
    "# We multiply by 252 to annualise the data - log_returns currently stores daily information\n",
    "\n",
    "# We also introduce our risk free rate variable, which will be needed in the sharpe ratio calculation\n",
    "\n",
    "risk_free_rate = 0.05"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nKTAkym5_qmv"
   },
   "source": [
    "We now want to analyse a portfolio more specifically. We want to look at 3 things - Expected return of a portfolio, volatility and sharpe ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1EjgWNtBAlKH"
   },
   "outputs": [],
   "source": [
    "def expected_portfolio_returns(weights, mean_returns):\n",
    "  return np.dot(weights, mean_returns)\n",
    "\n",
    "def volatility(weights, mean_returns, covariance_matrix):\n",
    "  return np.sqrt(np.dot(weights.T,np.dot(covariance_matrix,weights))) #standard deviation\n",
    "\n",
    "def sharpe_ratio(weights, mean_returns, covariance_matrix, risk_free_rate):\n",
    "  return (expected_portfolio_returns(weights, mean_returns) - risk_free_rate)/volatility(weights, mean_returns, covariance_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BvUbkLIIFeAw"
   },
   "source": [
    "Now, minimizing the negative sharpe ratio (to maximize the sharpe ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HhOBl35JFhwl"
   },
   "outputs": [],
   "source": [
    "def neg_sharpe(weights, mean_returns, covariance_matrix, risk_free_rate):\n",
    "    return -sharpe_ratio(weights, mean_returns, covariance_matrix, risk_free_rate)\n",
    "\n",
    "max_stock_percentage = 0.4 # maximum percentage of our portfolio that can be put on a single stock, to ensure that we don't put all our money into one stock\n",
    "num_of_stocks = len(tickers)\n",
    "constraints = ({'type': 'eq', 'fun': lambda w: np.sum(w) - 1})\n",
    "bounds = [(0,max_stock_percentage) for i in range(num_of_stocks)]\n",
    "\n",
    "initial_weights = [1/num_of_stocks for i in range(num_of_stocks)]\n",
    "\n",
    "optimised_results = minimize(neg_sharpe,initial_weights, args = (mean_returns, covariance_matrix, risk_free_rate), method = \"SLSQP\", bounds = bounds, constraints = constraints)\n",
    "optimal_weights = optimised_results.x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tRf4-VnX_c8N"
   },
   "source": [
    "Analysing the portfolio using these optimal weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cbfLdYnmASKm",
    "outputId": "0986fb28-5955-409e-c2f0-93837367ac2e"
   },
   "outputs": [],
   "source": [
    "print(\"Optimal weights for portfolio\")\n",
    "for ticker, weight in zip(tickers, optimal_weights):\n",
    "  print(f\"{ticker} weight: {weight:.3f}\")\n",
    "\n",
    "optimal_portfolio_return = expected_portfolio_returns(optimal_weights, mean_returns)\n",
    "optimal_portfolio_volatility = volatility(optimal_weights, mean_returns, covariance_matrix)\n",
    "optimal_sharpe_ratio = sharpe_ratio(optimal_weights, mean_returns, covariance_matrix, risk_free_rate)\n",
    "\n",
    "print(f\"Expected Annual Return: {optimal_portfolio_return:.3f}\")\n",
    "print(f\"Expected Volatility: {optimal_portfolio_volatility:.3f}\")\n",
    "print(f\"Sharpe Ratio: {optimal_sharpe_ratio:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
